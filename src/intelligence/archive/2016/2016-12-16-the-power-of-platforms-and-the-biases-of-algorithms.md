---
layout: layouts/intelligence-post.njk
title: 'The Power of Platforms and the Biases of Algorithms'
date: 2016-12-16
description: 'Part five in an ongoing series'
tags:
  - 'Medium'
  - 'Analysis'
  - 'Platforms'
  - 'Biases'
  - 'algorithms'
  - 'Part'
  - 'Five'
medium_id: '706300732630'
---
<section name="0a61" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><figure name="4156" id="4156" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*m3K73gt3lnYIDPSaDChZcw.jpeg" data-width="4032" data-height="3024" src="/assets/medium/2016-12-16-the-power-of-platforms-and-the-biases-of-algorithms/1-m3K73gt3lnYIDPSaDChZcw-eebf31b2.jpeg"><figcaption class="imageCaption">Photo of a powerful platform by Jesse Hirsh</figcaption></figure><p name="b290" id="b290" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Part five in an ongoing series</em></strong></p><p name="91a0" id="91a0" class="graf graf--p graf-after--p">Algorithmic media have a kind of power, however this power largely remains invisible to the audience or user who instead interact with the information that the algorithm sorts and delivers (<a href="http://dl.acm.org/citation.cfm?id=2697079&amp;dl=ACM&amp;coll=DL&amp;CFID=703310142&amp;CFTOKEN=29196076" data-href="http://dl.acm.org/citation.cfm?id=2697079&amp;dl=ACM&amp;coll=DL&amp;CFID=703310142&amp;CFTOKEN=29196076" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Tufekci, 2015</a>). Where this power manifests is on platforms that make extensive use of algorithms to draw and consolidate the attention of audiences.</p><p name="ae6f" id="ae6f" class="graf graf--p graf-after--p"><a href="https://roughdraft.review/are-algorithms-institutions-5cfba851fb20" data-href="https://roughdraft.review/are-algorithms-institutions-5cfba851fb20" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Using institutional theory</a> (which regards algorithms as institutions), <a href="https://roughdraft.review/are-algorithms-enabling-autonomous-audiences-7464e5bda663" data-href="https://roughdraft.review/are-algorithms-enabling-autonomous-audiences-7464e5bda663" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">automodernity</a> (which acknowledges the agency we bring to our use of algorithms), and the <a href="https://roughdraft.review/why-democratic-publics-matter-in-the-age-of-algorithms-168446169a34" data-href="https://roughdraft.review/why-democratic-publics-matter-in-the-age-of-algorithms-168446169a34" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">concept of algorithmic publics</a> (the spaces created by the use of algorithmic media), we can start to map out a growing and potential field of research that attempts to measure and explore the growing role and influence of algorithmic media on audiences and therefore society. Especially when employed by platforms like Facebook, Twitter, and Google.</p><p name="00d9" id="00d9" class="graf graf--p graf-after--p"><a href="https://communication.cals.cornell.edu/people/tarleton-gillespie" data-href="https://communication.cals.cornell.edu/people/tarleton-gillespie" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Tarleton Gillespie</a> (<a href="http://web.mit.edu/comm-forum/mit6/papers/Gillespie.pdf" data-href="http://web.mit.edu/comm-forum/mit6/papers/Gillespie.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">2010</a>) is a vocal critic of the evolving relationship between the new media industries and their audiences, in particular taking issue with the use of the word “platform”, arguing that it provides a false vision of “technical neutrality and progressive openness” (360) when the reality is quite the opposite. Rather than regard platforms as a level playing field we should instead regard them as what they are, slanted towards the interests of the owner, serving specific commercial needs and interests.</p><p name="0473" id="0473" class="graf graf--p graf-after--p">Gillespie argues in a later paper (2015) that platforms exercise considerable power in their ability to make interventions, whether that involves shaping how the platforms are used, or more importantly deleting content or users of the platform itself. The owners and developers of these platforms embed the logic of commercialism, and promote an ongoing narcissistic self-promotion as users compete for attention and audience.</p><p name="d59f" id="d59f" class="graf graf--p graf-after--p">These interventions, in particular the deletions of content and suspensions of accounts has a hidden but tangible impact on the public culture that forms around these platforms. They shape the way we use these platforms, and they establish social norms around what is and what is not acceptable use. Other researchers go even further as to argue that these platforms alter our perception of time, creating a “realtimeness” that is a byproduct of the never ending feeds of new content and information (<a href="http://www.annehelmond.nl/wordpress/wp-content/uploads/2014/06/WeltevredeHelmondGerlitz_Real-time.pdf" data-href="http://www.annehelmond.nl/wordpress/wp-content/uploads/2014/06/WeltevredeHelmondGerlitz_Real-time.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Weltevrede et al, 2014</a>; <a href="https://www.researchgate.net/publication/280771930_Facebook_time_Technological_and_institutional_affordances_for_media_memories" data-href="https://www.researchgate.net/publication/280771930_Facebook_time_Technological_and_institutional_affordances_for_media_memories" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Kaun et al, 2014</a>).</p><h3 name="b8c2" id="b8c2" class="graf graf--h3 graf-after--p">Emotional Manipulation</h3><p name="aa8d" id="aa8d" class="graf graf--p graf-after--h3">Perhaps one of the most controversial studies that illustrates the power of selection on social media was one that examined the transfer of emotional states via emotional contagion (<a href="http://www.pnas.org/content/111/24/8788.full" data-href="http://www.pnas.org/content/111/24/8788.full" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Kramer et al, 2014</a>). Facebook users who were shown positive posts, proceeded to make positive posts themselves. Users with negative posts then made similarly negative posts. While the study has faced considerable debate over its methods and ethics, it clearly demonstrates the power these platforms have (<a href="https://philpapers.org/rec/BOYURA" data-href="https://philpapers.org/rec/BOYURA" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">boyd, 2015</a>). In particular their ability to leverage peer pressure, i.e. our desire to do as our friends do.</p><p name="3a5c" id="3a5c" class="graf graf--p graf-after--p">This emotional contagion experiment is also the rare instance in which Facebook shared the results of the ongoing research they conduct on their users. Whether their goal is optimizing their interface, or introducing new reactions beyond the like button, Facebook is constantly experimenting on their users, watching how they react to different content and stimuli (<a href="https://www.bloomberg.com/features/2016-facebook-reactions-chris-cox/" data-href="https://www.bloomberg.com/features/2016-facebook-reactions-chris-cox/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Frier, 2016</a>). An algorithmic platform is by definition an ongoing experiment upon its users (<a href="http://motherboard.vice.com/en_uk/read/twitters-new-algorithmic-timeline-isnt-about-usability-its-about-tracking-your-behavior" data-href="http://motherboard.vice.com/en_uk/read/twitters-new-algorithmic-timeline-isnt-about-usability-its-about-tracking-your-behavior" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Assar, 2016</a>). Which is precisely why algorithmic opaqueness is an issue. As users, as subjects, we have the right to know how we are being experimented upon.</p><p name="8152" id="8152" class="graf graf--p graf-after--p">We place so much trust and blind faith in technology and algorithms that Gillespie also asks “<a href="http://limn.it/can-an-algorithm-be-wrong/" data-href="http://limn.it/can-an-algorithm-be-wrong/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Can an Algorithm be Wrong?</a>” (2012) citing the instance of Twitter suppressing the #OccupyWallStreet hashtag in the early days of that particular social movement. This was an example of the power that algorithms have to determine what is not only worthy of the public’s attention, but also, supposedly, reflects the will of the people (Gillespie, 2012). Thus when a social movement is suppressed by an algorithm, as happened in this case, it raises the question of where else such algorithmic discrimination may occur?</p><h3 name="b763" id="b763" class="graf graf--h3 graf-after--p">Recognizing the bias of Algorithms</h3><p name="d760" id="d760" class="graf graf--p graf-after--h3"><a href="http://solon.barocas.org/" data-href="http://solon.barocas.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Solon Barocas</a> (<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2536579" data-href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2536579" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">2014</a>) is a scholar who focuses on algorithmic biases, their impact on privacy (<a href="http://dl.acm.org/citation.cfm?id=2668897" data-href="http://dl.acm.org/citation.cfm?id=2668897" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">2014</a>), as well as the way in which the biases embedded in algorithms can impact their application in big data (<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2477899" data-href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2477899" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">2016</a>). However, on a broader level Barocas is also articulating a framework to describe how algorithms govern our lives (<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2245322" data-href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2245322" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">2013</a>).</p><p name="29e3" id="29e3" class="graf graf--p graf-after--p">Whether as a myth, an interface to how we consume information, or literal rules that sort and make decisions about us, the way in which these algorithms govern us is growing (<a href="https://www.researchgate.net/publication/282970489_Governing_Algorithms_Myth_Mess_and_Methods" data-href="https://www.researchgate.net/publication/282970489_Governing_Algorithms_Myth_Mess_and_Methods" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Ziewitz, 2015</a>). Solon Barocas is also attempting to articulate a regulatory response to algorithmic biases which may assist in the development of laws that would prevent algorithmic discrimination.</p><p name="7e46" id="7e46" class="graf graf--p graf-after--p">However, in order to articulate a regulatory response, we need to also understand the environment that would be subject to regulation. The growing role of algorithms when it comes to governance and the way in which they make decisions about us is spreading rapidly. Unfortunately, these systems are being developed and deployed within a technocratic or engineer centric perspective that pays little attention to context.</p><p name="6169" id="6169" class="graf graf--p graf-after--p"><a href="http://www.katecrawford.net/" data-href="http://www.katecrawford.net/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Kate Crawford</a> (<a href="http://www.katecrawford.net/docs/CanAnAlgorithmBeAgonistic-April2016.pdf" data-href="http://www.katecrawford.net/docs/CanAnAlgorithmBeAgonistic-April2016.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">2016</a>) seeks to counter this by promoting an agonistic pluralism that recognizes algorithms (and their platforms) do not operate in a vacuum, but are part of society and impact society. They are not purely technological constructs, but social constructs that impact our behaviour while making judgements about us.</p><p name="0eac" id="0eac" class="graf graf--p graf-after--p">Similarly, social media scholar <a href="http://www.danah.org/" data-href="http://www.danah.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">danah boyd</a> notes that the networked nature of algorithmic discrimination (<a href="http://www.danah.org/papers/2014/DataDiscrimination.pdf" data-href="http://www.danah.org/papers/2014/DataDiscrimination.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">boyd et al, 2014</a>) is a social phenomenon, in that we may be judged as individuals, and yet we use these platforms for social means, and have associations that say more about us than what we provide on our own, vis a vis personal information:</p><p name="2cbe" id="2cbe" class="graf graf--p graf--startsWithDoubleQuote graf-after--p">“We live in a highly networked world, in which our social connections can operate as both help and hindrance. For some people, ‘who you know’ is the key to getting jobs, or dates, or access to resources; for others, social and familial connections mean contending with excessive surveillance, prejudice, and ‘guilt by association.’” (54)</p><p name="b0f1" id="b0f1" class="graf graf--p graf-after--p">Certainly guilt by association in the form of racial profiling is something that many communities are intimately familiar with and therefore it is interesting to note racial discrimination when it comes to online advertising (<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2208240" data-href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2208240" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Sweeney, 2013</a>). The same advertisement will be displayed differently, depending upon the perceived race of the subject, in certain cases assuming criminal association and background.</p><p name="cbb6" id="cbb6" class="graf graf--p graf-after--p">In response, there are attempts to create algorithms that focus on fairness, which includes racial fairness so that these sorts of biases are not part of the user experience or system operation (<a href="https://arxiv.org/abs/1104.3913" data-href="https://arxiv.org/abs/1104.3913" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Dwork et al, 2011</a>). However even these notions of fairness begin with ideological assumptions around power and inequality.</p><p name="0f53" id="0f53" class="graf graf--p graf-after--p">The design of algorithms can attempt to compensate for the biases of society, but in so doing, create their own. Algorithmic fairness can only work in a transparent environment, which most algorithms do not adhere to, given their complex and often confusing nature. There are methods being developed that seek ways outside of transparency to detect or discover algorithmic discrimination (<a href="http://www-personal.umich.edu/~csandvig/research/Auditing%20Algorithms%20--%20Sandvig%20--%20ICA%202014%20Data%20and%20Discrimination%20Preconference.pdf" data-href="http://www-personal.umich.edu/~csandvig/research/Auditing%20Algorithms%20--%20Sandvig%20--%20ICA%202014%20Data%20and%20Discrimination%20Preconference.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Sandvig et al, 2014</a>), however they are interim strategies that help support arguments in favour of transparency.</p><p name="ae77" id="ae77" class="graf graf--p graf-after--p"><a href="https://twitter.com/FrankPasquale" data-href="https://twitter.com/FrankPasquale" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Frank Pasquale</a> a <a href="https://www.law.umaryland.edu/faculty/profiles/faculty.html?facultynum=984" data-href="https://www.law.umaryland.edu/faculty/profiles/faculty.html?facultynum=984" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">law professor from the University of Maryland</a> also addresses how algorithms discriminate and the need for them to be fair in his book “<a href="http://www.hup.harvard.edu/catalog.php?isbn=9780674368279" data-href="http://www.hup.harvard.edu/catalog.php?isbn=9780674368279" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">The Black Box Society</a>” (2015). Pasquale not only looks at how algorithms shape our society, but also how they shape ourselves, how we bend to their logic, in order to find work, to find love, and in so doing conform to what the algorithm wants us to be, a/k/a our algorithmic self (<a href="http://www.iasc-culture.org/THR/THR_article_2015_Spring_Pasquale.php" data-href="http://www.iasc-culture.org/THR/THR_article_2015_Spring_Pasquale.php" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">2015</a>). He also points out that all algorithms lead to scoring and therefore hierarchies, and that these emerging kinds of social status are not only invisible, but so too are the biases and ideologies that drive them.</p><p name="982b" id="982b" class="graf graf--p graf-after--p">However, what Pasquale fails to address is the agency we feel while using algorithms (vis a vis automodernity), and the way in which algorithms, as media, create publics. His legalistic approach leaves out the cultural impact and consequently the way we experience and engage algorithmic media.</p><h3 name="e35b" id="e35b" class="graf graf--h3 graf-after--p">We Are Defined by Algorithms</h3><p name="1a93" id="1a93" class="graf graf--p graf-after--h3">A great illustration of the notion of the algorithmic self is the work of <a href="http://adriennemassanari.com/" data-href="http://adriennemassanari.com/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Adrienne Massanari</a>, a researcher and professor at the University of Illinois at Chicago who studies Reddit, a social-news and community site that has considerable online influence and power. Massanari writes about how the Reddit algorithm and design implicitly support anti-feminist and misogynist cultures (<a href="https://www.researchgate.net/publication/283848479_Gamergate_and_The_Fappening_How_Reddit%27s_algorithm_governance_and_culture_support_toxic_technocultures" data-href="https://www.researchgate.net/publication/283848479_Gamergate_and_The_Fappening_How_Reddit&#39;s_algorithm_governance_and_culture_support_toxic_technocultures" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Massanari, 2015</a>).</p><p name="f1d7" id="f1d7" class="graf graf--p graf-after--p">The logic of the site rewards the kind of activity and behaviour that trolls engage in. The algorithm helps to cultivate the toxicity, when instead it could and should be designed to do the opposite. It also enables the hierarchy that thrives on reddit, that allows power users to game the system and send content to the front page due to their high standing with regard to the platform and the algorithm.</p><p name="c86c" id="c86c" class="graf graf--p graf-after--p"><a href="http://www.tedstriphas.com/" data-href="http://www.tedstriphas.com/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Ted Striphas</a> is attempting to articulate a broader algorithmic culture (<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2657752" data-href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2657752" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">2015</a>) and in building up a theoretical and cultural basis for the rise of algorithms, worries about the loss of publicness and finds instead the emergence of an elite culture. His concern is that algorithms isolate audiences, and elevate elites, those most valued in an audience, to a distinct status, while relegating everyone else to lower levels. Is this meant to suggest the re-emergence of a class system or a high and low brow culture? Or the erosion of the notion of publics?</p><p name="f251" id="f251" class="graf graf--p graf-after--p">It certainly reinforces the notion that hierarchies are a byproduct of algorithmic media, as the software sorts through audience members and values their contributions and reach accordingly. Micro celebrities are a growing phenomena as these platforms give some users the ability to grow audiences and with it an emerging social power (<a href="https://uncch.pure.elsevier.com/en/publications/not-this-one-social-movements-the-attention-economy-and-microcele" data-href="https://uncch.pure.elsevier.com/en/publications/not-this-one-social-movements-the-attention-economy-and-microcele" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Tufekci, 2013</a>).</p><p name="d8b6" id="d8b6" class="graf graf--p graf-after--p">However, we remain stuck in a false neutrality, where not only do we deny the power these platforms have, we also ignore the emerging elite that is a byproduct of how these platforms operate and their embedded ideologies.</p><p name="9d02" id="9d02" class="graf graf--p graf-after--p"><a href="http://www.lse.ac.uk/media@lse/WhosWho/AcademicStaff/robinMansell.aspx" data-href="http://www.lse.ac.uk/media@lse/WhosWho/AcademicStaff/robinMansell.aspx" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Robin Mansell</a> (<a href="http://eprints.lse.ac.uk/61318/1/__lse.ac.uk_storage_LIBRARY_Secondary_libfile_shared_repository_Content_Mansell,R_Platforms%20power_Mansell_Platform%20power_2015_cover.pdf" data-href="http://eprints.lse.ac.uk/61318/1/__lse.ac.uk_storage_LIBRARY_Secondary_libfile_shared_repository_Content_Mansell,R_Platforms%20power_Mansell_Platform%20power_2015_cover.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">2015</a>) argues that platforms are inherently political, biased, and specifically require a regulatory response that is “as innovative as the digital platform industry” (23).</p><p name="9ac4" id="9ac4" class="graf graf--p graf-after--p">What does this entail however? Algorithms regulating algorithms? How can regulatory agencies be as innovative as the platforms they might seek to oversee?</p><p name="2030" id="2030" class="graf graf--p graf-after--p">While there are modest attempts by regulatory agencies like the FTC in the US and Competition Bureau in Canada to monitor micro celebrities and their growing endorsement business, there is little attention otherwise placed on how regulators as agencies can and should adapt.</p><p name="d7bc" id="d7bc" class="graf graf--p graf-after--p">All this research opens a range of questions that interrogate the power and influence of algorithms, especially with regard to media audiences.</p><p name="a35b" id="a35b" class="graf graf--p graf-after--p">The false neutrality that is associated with platforms and technology is a dangerous kind of ignorance. Agnotology is an emerging field that studies the making and unmaking of ignorance (<a href="http://www.sup.org/books/title/?id=11232" data-href="http://www.sup.org/books/title/?id=11232" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Proctor, 2008</a>). Algorithmic media are shrouded in a kind of agnotology of their own as we willingly ignore the power of their platforms and the biases of their logic. However, agnotology, as a “sociology of things that aren’t there” (<a href="https://arizona.pure.elsevier.com/en/publications/agnotology-ignorance-and-absence-or-towards-a-sociology-of-things" data-href="https://arizona.pure.elsevier.com/en/publications/agnotology-ignorance-and-absence-or-towards-a-sociology-of-things" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Croissant, 2014</a>) does provide an interesting method for attempting to understand the role that algorithms play. At the very least it helps us begin to map out what we don’t know in a way that may lead us to what we need to know (<a href="https://www.ncbi.nlm.nih.gov/pubmed/22718476" data-href="https://www.ncbi.nlm.nih.gov/pubmed/22718476" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Weiss, 2012</a>).</p><p name="ddbe" id="ddbe" class="graf graf--p graf-after--p">One example of agnotology is the ongoing rise of algorithmic authority.</p><p name="2ace" id="2ace" class="graf graf--p graf-after--p graf--trailing"><a href="https://roughdraft.review/algorithmic-authority-netflix-uber-and-bitcoin-ad2a84ff5a9" data-href="https://roughdraft.review/algorithmic-authority-netflix-uber-and-bitcoin-ad2a84ff5a9" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Continued in Part 6</em></strong></a></p></div></div></section>
